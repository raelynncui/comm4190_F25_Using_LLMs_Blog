{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f57fb830-bf1b-4a01-96e7-46613fbc1358",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"LLMs and 4-Digit Multiplication\"\n",
    "description: \"Something that was impossible 2 years ago\" \n",
    "author: \"Raelynn Cui\"\n",
    "date: \"12/6/2025\"\n",
    "categories:\n",
    "  - ChatGPT\n",
    "  - LLMs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3aaf24-ee8a-4e98-b254-52e4f74498f0",
   "metadata": {},
   "source": [
    "# LLMs and 4-Digit Multiplication\n",
    "\n",
    "<img src=\"cover.png\" width=\"100%\"/>\n",
    "\n",
    "Recently, I came across this [Reddit thread](https://www.reddit.com/r/LocalLLaMA/comments/1bpj4to/what_are_some_promptstasks_you_dont_believe_state/) from 2 years ago:\n",
    "\n",
    "<img src=\"reddit.png\" width=\"60%\"/>\n",
    "\n",
    "One response to this post was the following:\n",
    "> what I call real reasoning : tasks where only the right premises lead to the right conclusions. One example is 4 by 4 digit multiplication. No LLM can do it and give always the correct results no matter how it's promoted (without giving it a explicit algorithm or resorting to using \"tools\").\n",
    "\n",
    "> LLM are unable to deal with compositionality which is required for these tasks.\n",
    "\n",
    "Since this post (and comment) were made two years ago, I was wondering how current LLMs measure up to this task. In this blog post, I'll compare the responses of GPT-4o (old) and GPT-5.1 (new). \n",
    "> compute 1823 x 9913 without using any tools.\n",
    "\n",
    "(The correct answer is 18,071,399, by the way)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6460bf8-e79e-4e0c-bf2e-98e6b8e10f51",
   "metadata": {},
   "source": [
    "## How do LLMs do math?\n",
    "\n",
    "I thought this was an important concept to cover -- because I'm telling the LLM to not use any tools (e.g. a calculator), these models have to generate the answer somehow -- but how?\n",
    "\n",
    "Because LLMs are next-token predictors, when you ask for a multiplication product, the model isn’t actually running math algorithms -- instead, itt’s predicting what a mathematically plausible answer would look like based on patterns it has seen in its training data. Some models are able to break down math problems into logical steps, but others lack this multi-step/chain of reasoning unless you explicitly force them to “think out loud.” Therefore, LLMs don't really \"know\" math, but they can approximate some mathematical structures pretty well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa98614-4a61-4104-a796-41581843a4f1",
   "metadata": {},
   "source": [
    "## GPT-5.1\n",
    "\n",
    "GPT-5.1 answered correctly:\n",
    "\n",
    "--- \n",
    "Let’s do this cleanly by hand, breaking the multiplication into digestible pieces rather than trying to hold everything in working memory.\n",
    "\n",
    "We want:\n",
    "\n",
    "**1823 × 9913**\n",
    "\n",
    "A friendly way is to rewrite 9913 as **10,000 − 87**, because multiplying by 10,000 is trivial and subtracting the correction term is easier than multiplying directly.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 1: Multiply 1823 × 10,000**\n",
    "\n",
    "Just append four zeros:\n",
    "\n",
    "**1823 × 10,000 = 18,230,000**\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 2: Compute the correction term: 1823 × 87**\n",
    "\n",
    "Break 87 into 100 − 13 (again to make mental handling easier):\n",
    "\n",
    "### **1823 × 100 = 182,300**\n",
    "\n",
    "### **1823 × 13**\n",
    "\n",
    "Compute 1823 × 13 as 1823 × (10 + 3):\n",
    "\n",
    "* 1823 × 10 = 18,230\n",
    "* 1823 × 3 = 5,469\n",
    "  Add them:\n",
    "  **18,230 + 5,469 = 23,699**\n",
    "\n",
    "So:\n",
    "\n",
    "### **1823 × 13 = 23,699**\n",
    "\n",
    "Now subtract that from the ×100 result:\n",
    "\n",
    "**182,300 − 23,699 = 158,601**\n",
    "\n",
    "Thus:\n",
    "\n",
    "### **1823 × 87 = 158,601**\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 3: Combine everything**\n",
    "\n",
    "Since 9913 = 10,000 − 87:\n",
    "\n",
    "**1823 × 9913 = 18,230,000 − 158,601**\n",
    "\n",
    "Now subtract carefully:\n",
    "\n",
    "18,230,000\n",
    "−   158,601\n",
    "= **18,071,399**\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **Final Answer: 18,071,399**\n",
    "\n",
    "If you want, I can also show a classical long-multiplication layout to verify it.\n",
    "\n",
    "---\n",
    "\n",
    "In this response, I was surpised that the LLM took the step to first compute 1823 × 10,000 and then subtract the offset. Going back to the previous part about LLMs not really \"knowing\" math, I feel like this breaking down of the problem really exemplifies that -- GPT-5.1 may not really \"know\" the computations, but it can identify and execute patterns (e.g. first computing 1823 × 10,000 and appending four zeros to perform this multiplication) that allow it to get to the right answer. \n",
    "\n",
    "\n",
    "\n",
    "## GPT-4o \n",
    "\n",
    "On the other hand, GPT-4o got the answer wrong:\n",
    "\n",
    "\n",
    "Let’s multiply 1823 × 9913 manually using long multiplication.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Break 9913 into digits\n",
    "\n",
    "We'll multiply 1823 by each digit of 9913, starting from the right:\n",
    "\n",
    "[\n",
    "\\begin{align*}\n",
    "&\\phantom{+}1823 \\times 3 = \\quad ? \\\n",
    "&\\phantom{+}1823 \\times 10 = \\quad 1823 \\times 1 \\text{ (shifted one place)} \\\n",
    "&\\phantom{+}1823 \\times 900 = \\quad 1823 \\times 9 \\text{ (shifted two places)} \\\n",
    "&\\phantom{+}1823 \\times 9000 = \\quad 1823 \\times 9 \\text{ (again, shifted three places)} \\\n",
    "\\end{align*}\n",
    "]\n",
    "\n",
    "---\n",
    "\n",
    "### Step 2: Multiply each digit\n",
    "\n",
    "1. **1823 × 3**\n",
    "   [\n",
    "   1823 \\times 3 = 5469\n",
    "   ]\n",
    "\n",
    "2. **1823 × 1 (tens place)**\n",
    "   Shift left by 1 digit:\n",
    "   [\n",
    "   1823 \\times 1 = 1823 \\rightarrow 18230\n",
    "   ]\n",
    "\n",
    "3. **1823 × 9 (hundreds place)**\n",
    "   [\n",
    "   1823 \\times 9 = 16407 \\rightarrow shift by two digits = 1640700\n",
    "   ]\n",
    "\n",
    "4. **1823 × 9 (thousands place)**\n",
    "   Same as above but shift three digits:\n",
    "   [\n",
    "   1823 × 9 = 16407 → shift = 16407000\n",
    "   ]\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3: Add them together\n",
    "\n",
    "[\n",
    "\\begin{align*}\n",
    "&\\phantom{+} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ 5469 \\\n",
    "+&\\phantom{ \\ \\ \\ \\ \\ \\ \\ \\ \\ }18230 \\\n",
    "+&\\phantom{ \\ \\ \\ \\ \\ \\ }1640700 \\\n",
    "+&\\phantom{ \\ \\ \\ \\ \\ }16407000 \\\n",
    "\\hline\n",
    "=&18072799\n",
    "\\end{align*}\n",
    "]\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Final Answer:\n",
    "\n",
    "[\n",
    "\\boxed{1823 \\times 9913 = 18,!072,!799}\n",
    "]\n",
    "\n",
    "---\n",
    "\n",
    "GPT-4o formatted its computation steps differently (making the Markdown a bit difficult to read), but its final answer was 18,072,799 -- which is incorrect. Whereas GPT-5.1 first broke the problem down into a simpler problem (1823 × 10,000), GPT-4o dove straight into doing the multiplication and ended up getting the incorrect answer.\n",
    "\n",
    "I wondered if GPT-4o could still arrive at the correct answer if I gave it the broken-down problem that GPT-5.1 generated for itself. So, I prompted it:\n",
    "> First, compute 1823 × 10,000. Then, compute 1823 × 87 by first computing 1823 × 100 and then subtracting 1823 × 13 from it. Finally, take the product of 1823 × 10,000 and subtract the product of 1823 × 87 from it.\n",
    "\n",
    "(This was the same line of reasoning that GPT-5.1 used)\n",
    "\n",
    "In response to this, it got the answer correct!\n",
    "\n",
    "Here was the full response: \n",
    "\n",
    "Let's walk through this step by step.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Compute** 1823 × 10,000\n",
    "\n",
    "Multiplying by 10,000 is just appending four zeros:\n",
    "\n",
    "[\n",
    "1823 \\times 10{,}000 = 18{,}230{,}000\n",
    "]\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Compute** 1823 × 87\n",
    "\n",
    "Break it down into:\n",
    "\n",
    "[\n",
    "1823 \\times 100 = 182{,}300\n",
    "]\n",
    "[\n",
    "1823 \\times 13 =\n",
    "\\begin{align*}\n",
    "1823 \\times 10 &= 18{,}230 \\\n",
    "1823 \\times 3  &= 5{,}469 \\\n",
    "\\text{Sum} &= 18{,}230 + 5{,}469 = 23{,}699\n",
    "\\end{align*}\n",
    "]\n",
    "\n",
    "Now subtract:\n",
    "\n",
    "[\n",
    "1823 \\times 87 = 1823 \\times (100 - 13) = 182{,}300 - 23{,}699 = 158{,}601\n",
    "]\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Subtract**\n",
    "\n",
    "[\n",
    "1823 \\times 10{,}000 - 1823 \\times 87 = 18{,}230{,}000 - 158{,}601 = \\boxed{18{,}071{,}399}\n",
    "]\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Final Answer: **18,071,399**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f481f552-13a5-4e56-8105-63ca0fab3c5a",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "Overall, I think that GPT-4o initially getting the answer wrong but then getting it correct after I prompted it to follow GPT-5.1's line of reasoning really exemplifies how LLMs work under the hood. In this 4-digit multiplication example, the performance of the LLM wasn't really about its ability to predict the next mathematical token the best -- rather, it was about whether or not it could break the problem down into smaller, more manageable steps that it could perform well on. Once I prompted GPT-4o to first do these simple steps, it was able to predict next tokens just as well as GPT-5.1 was. \n",
    "\n",
    "This goes to show how important \"thinking\" is in LLMs. If you can force the LLM to break down the problem, it's setting itself up for success. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
