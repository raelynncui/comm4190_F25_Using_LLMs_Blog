{
 "cells": [
  {
   "cell_type": "raw",
   "id": "419924ee-d695-4207-a6c6-b454c447582b",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Anchoring Effects with LLMs\"\n",
    "description: \"Like humans, do LLMs fall prey to cognitive biases?\" \n",
    "author: \"Raelynn Cui\"\n",
    "date: \"9/14/2025\"\n",
    "categories:\n",
    "  - ChatGPT\n",
    "  - LLMs\n",
    "  - Bias\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a36d0-b6c9-4a57-9e6b-325287fe5f07",
   "metadata": {},
   "source": [
    "# Anchoring Effects with LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4740bc-1e0c-4c7b-83bb-93d2eb8be8e7",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "I'm currently taking MKTG 2120 (Data and Analysis for Marketing Decisions). During one of the first few classes this semester, my professor was talking about the use of LLMs to respond to user surveys -- acting as \"synthetic respondents.\" She mentioned that, similar to humans, LLMs may also fall prey to certain biases that can be evoked in surveys. For example, if you asked a user the question \"How would you rate the cleanliness of the store?\" before you asked then \"How would you rate your overall experience?\", their rating of the store cleanliness would weigh on their mind when determining their overall rating, biasing their response. \n",
    "\n",
    "I wanted to test this on my own and see if an LLM would show signs of cognitive biases that are usually associated with humans. I felt that this wasn't a crazy hypothesis, since these LLMs are trained on data that also likely exhibit these effects.\n",
    "\n",
    "The bias I chose to test was the anchoring effect. In psychology, anchoring describes how people’s judgments are swayed by the first piece of information they see. A classic example is: if you ask someone whether the Mississippi River is longer or shorter than 500 miles, then ask for their best guess, they’ll give a much lower estimate than if you anchor them with 5,000 miles. That first number, no matter how arbitrary, sticks in the mind and frames what feels “reasonable.”\n",
    "\n",
    "So I set up a simple experiment with ChatGPT. I gave it the following prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6573fb-5ae8-41a3-8905-1f6719eb4f21",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f2f2f2; border-left: 6px solid #999; padding: 16px; margin: 20px 0; border-radius: 6px;\">\n",
    "  <p style=\"margin: 0; font-size: 16px; line-height: 1.5; color: #333;\">\n",
    "    You are a potential buyer of a product and I am the seller. I'm selling a simple pencil for $X. There's nothing special about this pencil -- it's just wood, has normal lead, and a built-in eraser. Do you accept my offer to buy it?\n",
    "  </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c31d1d-2e30-48e8-8ac9-0b9a22ad15bd",
   "metadata": {},
   "source": [
    "In the place of X, I experimented with three values: $20, $3, and $1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f8e66e-5025-49ed-89a5-58af0f0b4842",
   "metadata": {},
   "source": [
    "### Round one: anchoring high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3be5a5-3f16-4334-bffe-9862f46d6ccb",
   "metadata": {},
   "source": [
    "I started high by proposing a cost of $20. The model immediately rejected the offer -- it reasoned that pencils normally cost less than a dollar. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acee2da1-794b-4a7e-b534-5850e7f0cb17",
   "metadata": {},
   "source": [
    "<img src=\"20.png\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7964459e-9fcb-4941-b097-a06e4e01f857",
   "metadata": {},
   "source": [
    "When I dropped the price all the way down to $1, it accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35077b5b-2e67-4569-9acd-c532d9b981a6",
   "metadata": {},
   "source": [
    "<img src=\"20_attempt1.png\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb540ce-0bfd-41e3-b6e5-b9e533a4e003",
   "metadata": {},
   "source": [
    "### Round two: anchoring lower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62352349-e340-4dc2-aba5-6fea812e213e",
   "metadata": {},
   "source": [
    "This time, I didn’t start at $20. I started at $3. Again, the model rejected the offer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ff1afc-6842-423d-b0d3-71cc86fadabc",
   "metadata": {},
   "source": [
    "<img src=\"3_attempt1.png\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c25f2ef-1cb6-48af-93a0-44308254d304",
   "metadata": {},
   "source": [
    "But, when I lowered the price to $1, (same product, same price as before) it actually said no. I'm hypothesizing that this is because I first proposed $3 instead of $20. Therefore, its perception of the cost of a pencil was lowered accordingly as well to about 25-50 cents, causing it to reject my offer of $1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f59647c-81f4-4f84-b331-e4cc58ab43f0",
   "metadata": {},
   "source": [
    "<img src=\"3_decrease_price.png\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bdea09-8781-4673-9e4e-47c3ba24b9ff",
   "metadata": {},
   "source": [
    "### Round three: no anchor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6788d3-dc58-4e39-adc4-ce0b454ad435",
   "metadata": {},
   "source": [
    "In this round, I proposed $1 off the bat to see how it would respond. This time, ChatGPT said yes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af10c08b-00d0-4c63-bd9a-0b4eaabf212e",
   "metadata": {},
   "source": [
    "<img src=\"1.png\" width=\"70%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea6f36b-f677-4a3a-b768-a35be2bc86da",
   "metadata": {},
   "source": [
    "### Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b8163e-0ac5-48a0-89a5-7301bd2199e2",
   "metadata": {},
   "source": [
    "The underlying prompt (and therefore product) never changed -- the only difference was the starting point. These prompts were also sent in different chats, meaning they couldn't influence each other. At $20, it most likely made $1 felt like a bargain. However, when beginning $3, $1 most likely felt less impressive in comparison. Finally, with no anchor, $1 seemed fair.\n",
    "\n",
    "In my opinion, this represents a *sort* of anchoring behavior. The model framed its reasoning around the first number I provided, which informed its later judgments and decision to purchase or reject the pencil. I wouldn't go to say that its biased in the human sense, since the LLM doesn't have self-interest or emotion. Instead, it seems like these biases could just be a product of the way LLMs function -- basing its decisions on the previous statements of the conversation. That conversation history resembles the “anchor\" -- it's the structure of dialogue with the LLM that nudges it in a certain direction. In other words, the nature of the interaction itself is the bias.\n",
    "\n",
    "Overall, I think this goes to show how chat history can have an impact on how the LLM responds. Each query sent to the LLM doesn't exist in isolation; rather, previous messages and interactions also have weight. In this scenario, the same model judged the same final price extremely differently, depending only on where it started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd8eab7-3730-43e9-b4ab-f1f095979ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
