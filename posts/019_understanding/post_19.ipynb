{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f44dc8d7-73d8-4dd0-9416-01bd20197ede",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Do LLMs Really Understand Language?\"\n",
    "description: \"A reflection on the discussion with Dr Marlon Twyman\" \n",
    "author: \"Raelynn Cui\"\n",
    "date: \"10/27/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49009be7-93a1-4874-930b-d57cc12c06f8",
   "metadata": {},
   "source": [
    "# Do LLMs Really Understand Language?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a16405-f458-4a6e-8798-c4dd7273230c",
   "metadata": {},
   "source": [
    "<img src=\"cover.jpg\" width=\"100%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ace276a-3333-4dca-8831-df701756f023",
   "metadata": {},
   "source": [
    "## Context\n",
    "During the class discussion with Dr Marlon Twyman, we discussed the idea of whether or not LLMs can really *understand* language. This debate came from a section of the paper \"On the Danger of Schoastic Parrots,\" where the authors discuss the linguistic concept of a \"garden path\" and how LLMs can lead us down this path to deceive us in believing that it can actually understand language. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd70202-0574-4dbb-a3d4-2c78cc68106f",
   "metadata": {},
   "source": [
    "<img src=\"paper.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb5b7cf-e6f8-4b73-807e-6103347f5f52",
   "metadata": {},
   "source": [
    "When the class was asked if they believed that LLMs were capable of true understanding, 8 said \"not sure\" and 8 said \"no.\" I was conflicted on this idea, bordering on the edge of \"yes\" and \"not sure.\" I've been wondering what others online think about this, so in this blog, I explore what others are saying about this topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2213c159-7aab-4bdc-8df3-eae38e5ba3da",
   "metadata": {},
   "source": [
    "## Sean Trott's \"The Counterfactual\"\n",
    "\n",
    "Sean Trott is an assistant professor at UC San Diego. His blog, The Counterfactual, explores LLMs and Cognitive Science. In this specific [article](https://seantrott.substack.com/p/how-could-we-know-if-large-language), he thinks about whether or not LLMs can truly *understand* and answers the question: \"can a language model have a mind?\"\n",
    "\n",
    "Trott's research areas are really interesting -- take a look [here](https://seantrott.github.io/research/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ffebb6-73b5-446b-8199-889f3aebb670",
   "metadata": {},
   "source": [
    "<img src=\"counterfactual.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bde33b-f1e7-4684-a645-308b7e4f0725",
   "metadata": {},
   "source": [
    "In this article, two main viewpoints are presented:\n",
    "- Axiomatic rejection view: LLMs cannot understand language because they rely only on linguistic form (without grounding in real-world meaning, intention, or cognition). This view draws from arguments like Searleâ€™s Chinese Room, ultimately arguing that behavior alone isn't sufficient to prove that the LLM is actually understanding.\n",
    "- Duck test view: If a model consistently behaves as though it understands language (by passing tests and responding in meaningful ways), then it should be considered as having some form of understanding. In this view, understanding is judged based on observable behavior.\n",
    "\n",
    "Personally, I can understand both of these views, and I think it comes down to how you define \"understanding\" to answer the question. Trott himself ultimately favors the duck test approach, seeming to argue that understanding should be empirically studied through measurable behaviors rather than ruled out by definition. \n",
    "\n",
    "I was surprised by this, since the duck test view feels like it would be the more unpopular view. I was also surprised that this article was written in 2022, which, given the speed of LLM advancements, feels like a long time ago. The capabilities of LLMs like ChatGPT have grown exponentially since then, so I expect that Trott would feel even more strongly about his claims now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fec83d-322d-47af-829c-e3c1159c00e2",
   "metadata": {},
   "source": [
    "## Reddit\n",
    "\n",
    "I found many interesting opinions that people were voicing on Reddit forums under the [ChatGPT subreddit](https://www.reddit.com/r/ChatGPT/comments/1ctypcj/do_llms_actually_understand_language_or_do_they/) and [LocalLLaMA subreddit](https://www.reddit.com/r/LocalLLaMA/comments/1gbmkl5/can_llms_understand_understanding_understanding/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f3494b-b774-4d9d-9adc-c71c577119d8",
   "metadata": {},
   "source": [
    "<img src=\"reddit.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b70ee3-2707-4969-b159-1d993050875c",
   "metadata": {},
   "source": [
    "One person said:\n",
    "> What CURRENT LLMs do...is roughly analogous to what just the language bits of the human brain do. If you analyze your own behavior you'll see most people do not \"think\" about their responses very much at all when having a conversation unless they're talking about something fairly novel they have to think about. When someone's just asking how you've been or whatever, you can pretty much just \"predict the next word\" and it enables you to respond very quickly without thinking about it much (or even while seamlessly thinking about something else entirely and not paying attention).\n",
    "\n",
    "This comment had the most upvotes, with someone replying that they \"hit the nail on human language.\" I thought this idea was interesting but a bit over-simplified -- even when we feel like we're responding to things automatically, saying that we're  just \"predicting the next word\" puts too much of a black box over our cognitive mechanisms. \n",
    "\n",
    "Someone else said:\n",
    "> I don't think there's anything to be gained from trying to discuss this without defining exactly what we mean by \"understand\". What exactly is going on when a human \"understands\" something in a significant way? Only then can we check if something similar is going on in the LLM.\n",
    "\n",
    "I completely agree with this comment -- our understanding of \"understanding\" varies too much to productively discuss this debate at hand. As we saw in Trott's article, viewpoints of what \"understanding\" means can differ significantly. Depending on the context and use case, we might care more about some definitions of understanding and less about others.\n",
    "\n",
    "In fact, one poster under the r/LocalLLaMA subreditt said:\n",
    "> I would like to propose here a simple definition of understanding. It is pragmatic, and sidesteps the issues of subjectivity that lead us down rabbit holes of circular argumentation. Let us speak functionally of what understanding is. Understanding is the product of learning.\n",
    "\n",
    "I thought this idea of understanding being \"the product of learning\" was really interesting at first, but the more I thought about it, I feel like this doesn't get us closer to a more concrete and standardized definition of understanding. By phrasing it as \"the product of learning,\" one might then ask -- \"what is defined as learning?\" For example, does an LLM updating its weights and being fine-tuned sufficient to count as \"learning?\" It is technically updating its \"beliefs\" and relationship between ideas, but some would argue that this would not count as true learning. \n",
    "\n",
    "Overall, I feel like this disconnect comes from trying to apply abstract concepts like understanding and learning to a very mechanical and concrete system like an LLM. Going forward, I'm interested in seeing if the progression of LLMs' capabilities actually impact and change peoples' conceptions of what understanding and learning actually mean!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8d620d-1536-4b76-8481-ec9abc43ab85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
