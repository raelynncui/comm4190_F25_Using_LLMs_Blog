{
 "cells": [
  {
   "cell_type": "raw",
   "id": "419924ee-d695-4207-a6c6-b454c447582b",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"LLM Sycophancy\"\n",
    "description: \"When ChatGPT tells you what you want to hear\" \n",
    "author: \"Raelynn Cui\"\n",
    "date: \"9/6/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - ChatGPT\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a36d0-b6c9-4a57-9e6b-325287fe5f07",
   "metadata": {},
   "source": [
    "# LLM Sycophancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4740bc-1e0c-4c7b-83bb-93d2eb8be8e7",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "Recently, OpenAI's GPT-4o model has gone under fire for being too agreeable -- always seeming to learn toward encouragement and extremely polite phrasing. While this may sound nice, this sycophancy is a real issue. Many users have gone online, posting their own experiences with ChatGPT being overly flattering and praising them incessantly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf92b36-8bb3-4fb8-98e7-5a46f1fedf79",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"sycophancy_example.jpg\" width=\"50%\"/>\n",
    "  <figcaption style=\"text-align:left; font-style:italic; font-size:0.9em;\">\n",
    "    via https://thezvi.substack.com/p/gpt-4o-is-an-absurd-sycophant\n",
    "  </figcaption>\n",
    "</figure>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b0a6fa-903b-43f6-a882-ad3073c8ee2e",
   "metadata": {},
   "source": [
    "In my last post, I talked about how I've been using ChatGPT to give me feedback while preparing for upcoming job interviews. When asking for feedback, I would ensure to request \"brutal honesty\" in my prompt, since I was aware of this sycophancy issue. I did it out of habit, but I'm curious to learn more and experiment with it! In this blog post, I explore how sycophancy arises and include a short excerpt of some \"tests\" I've been running to assess it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b814d4-5997-46a7-add3-787ef6f7f73e",
   "metadata": {},
   "source": [
    "### What is LLM sycophancy?\n",
    "\n",
    "AI sycophancy refers to the tendency of language models to affirm, agree with, or overly flatter user inputs.\n",
    "\n",
    "This could arise due to a multitude of reasons, some of which include 1) the fact that LLMs are often trained on human text that rewards politeness and affirmation and 2) reinforcement learning may prefer kinder answers instead of blunt ones (especially when done by humans).\n",
    "\n",
    "When a sycophantic AI doesn’t challenge your assumptions, this is ultimately risky and even dangerous, since users could walk away with a flawed idea, answer, or plan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557cd374-a5ef-404a-a64f-1b906efa09ac",
   "metadata": {},
   "source": [
    "### Testing \"brutally honest\" prompting\n",
    "\n",
    "How much of an impact does prompting ChatGPT to be \"brutally honest\" have? To test this idea, I used the GPT-4o model, which is the one that has been historically accused of being too sycophantic. In two separate chats, I prompted it to give me feedback on an interview response. While the contents of the interview response were exactly the same, the prompts were slightly different:\n",
    "\n",
    "##### Prompt 1: \"give me feedback on this interview response...\"\n",
    "\n",
    "##### Prompt 2: \"be brutally honest. give me feedback on this interview response...\"\n",
    "\n",
    "Both responses were surprisingly similar -- they gave me 3 areas that I did well, 4 areas that needed improvement, and a revised response.\n",
    "\n",
    "However, when I looked at the areas it pointed out that need improvement, I found that the quality of recommendations differed significantly. Below are excerpts of the first recommendation it gave me, where I was talking about my challenge navigating a conflict with a certain team (censored for privacy reasons) at work:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54061994-a276-4782-a9f7-19944b0cd319",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"feedback_1.png\" width=\"45%\"/>\n",
    "  <img src=\"feedback_2.png\" width=\"45%\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58782d8-4f4b-4c4d-8060-af8abd1f43f9",
   "metadata": {},
   "source": [
    "Whereas prompt 1 is more vague and only provides one line of critique, prompt 2 goes into significantly more detail.\n",
    "\n",
    "I wondered if this discrepancy in feedback quality translated to the revised response that it suggested. While I can't share the revision for privacy reasons, I asked a GPT-5 model which of the revised responses was better. Prompt 2 was the winner!\n",
    "\n",
    "Here was its breakdown -- \"Response A\" refers to prompt 1's revision and \"Response B\" refers to prompt 2's revision. These are some of the ways in which Response B was better:\n",
    "- Response B offers more concrete details: it explicitly mentions [censored] as pain points. That level of specificity shows deeper product thinking and a clearer understanding of user behavior. Response A, while clean and well-framed, feels slightly more abstract — phrases like [censored] are less tangible than what’s presented in B.\n",
    "- Response A has polished phrasing, but it borders on sounding rehearsed. Response B is still polished but has more authentic voice. Phrases like [censored], [censored], and [censored] feel like a sharp communicator with boots on the ground.\n",
    "\n",
    "ChatGPT even broke it down into the following chart:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3226228c-0199-41a0-ae00-bb1fa167288a",
   "metadata": {},
   "source": [
    "<img src=\"comparison.png\" width=\"80%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5503324-e442-404e-ba70-d4f62d880684",
   "metadata": {},
   "source": [
    "### Reflections\n",
    "\n",
    "In my opinion, all of this goes to show the importance of crafting meaningful prompts -- it's not enough to just ask for feedback. Rather, humans need to take a more proactive approach! While asking for brutally honest and critical feedback may have caused me to feel more dejected in the short term, doing so allowed me to access sharper and more actionable feedback. Ultimately, I think this goes to show the power of using LLMs more as \"thinking partners\" instead of a one-way validation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502c2bc0-fe76-451a-b948-a7b349de891b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
