[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "COMM 4190 Blog",
    "section": "",
    "text": "How Simple Can You Get?\n\n\n\nChatGPT\n\nLLMs\n\n\n\nAsking ChatGPT to simplify a concept down to its simplest form\n\n\n\n\n\nSep 12, 2025\n\n\nRaelynn Cui\n\n\n\n\n\n\n\n\n\n\n\n\nEndless Image Replication\n\n\n\nChatGPT\n\nImage Generation\n\n\n\nWhat happens when you tell an image generation model to repeatedly replicate an image?\n\n\n\n\n\nSep 9, 2025\n\n\nRaelynn Cui\n\n\n\n\n\n\n\n\n\n\n\n\nUI Vibe Coding\n\n\n\nVibe Coding\n\nChatGPT\n\nClaude\n\nLovable\n\nUI/UX\n\n\n\nComparing UI replication abilities across ChatGPT, Claude, and Lovable\n\n\n\n\n\nSep 7, 2025\n\n\nRaelynn Cui\n\n\n\n\n\n\n\n\n\n\n\n\nLLM Sycophancy\n\n\n\nLLMs\n\nChatGPT\n\n\n\nWhen ChatGPT tells you what you want to hear\n\n\n\n\n\nSep 6, 2025\n\n\nRaelynn Cui\n\n\n\n\n\n\n\n\n\n\n\n\nChatGPT as a Mock Interviewer\n\n\n\nLLMs\n\nChatGPT\n\nProfessional\n\n\n\nUsing AI to prepare for behavioral interviews\n\n\n\n\n\nSep 5, 2025\n\n\nRaelynn Cui\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/004_image_replication/post_4.html",
    "href": "posts/004_image_replication/post_4.html",
    "title": "Endless Image Replication",
    "section": "",
    "text": "Endless Image Replication\n\nContext\nIn this blog, I play around with OpenAI‚Äôs GPT-5 image generation capabilities. Beginning with a photo of my dog, I repeatedly give the prompt ‚Äúcreate a replica of this image. don‚Äôt change a thing‚Äù, feeding the output of each iteration as the input of the next.\n\nI was inspired by this Reddit post, where a user shows how an image of a person became completely unrecognizable after the 74th iteration! My experiment didn‚Äôt go that far though, yielding different (and a little more underwhelming) results.\n\n\n\nResults\nI performed 11 iterations of the prompt using the GPT-5 model. Here‚Äôs an animation of my results:\n\nThe first iteration was extremely good ‚Äì the original image is on the left and the output is on the right. I initially had high hopes, since there are no noticeable differences I can spot when seeing the images side by side.\n\n\n\n\n\n\n\n\n\n\nHowever, it went quickly downhill after that. The biggest jump seemed to be within the third and fourth images, where the fourth image suddenly gained lots of noise and a higher contrast/sharpness.\n\n\n\n\n\n\n\n\n\n\nAfter that, it seemed to hit a plateau where the image became black and white and extremely noisy, with the outline of my dog still slightly distinguishable. This was the final photo after 11 rounds:\n\n\n\n\n\nReflections\nI couldn‚Äôt find any detailed and definitive source online regarding how the GPT-5 model generates images, but I‚Äôm assuming what happened here is that some noise/mismatches got introduced at some stage, which then got amplified as the iterations continued. According to an article on The Verge, the GPT-4o model uses an autoregressive approach (not diffusion), where it generates an image token by token, just like text. Instead of starting with random noise, like with diffusion, it predicts a sequence of image tokens that make up the image. Because of this, I‚Äôm surprised that the model introduced so much static/noise and didn‚Äôt veer off the path like the Reddit example.\nIn fact, I even fed the final image into GPT-5 and asked if it could identify what it was. It said: ‚ÄúThis is an image of a dog lying in the grass. The photo appears to have been processed with a strong filter or edge-detection effect, which makes it look like a high-contrast sketch or engraving. You can still make out the dog‚Äôs head, body, and the surrounding grass and trees in the background, though the details are stylized.‚Äù Therefore, it‚Äôs puzzling as to why it didn‚Äôt retain a more obvious ‚Äúdog token‚Äù in its image generation process.\nTo push the model a bit further, I asked it to ‚Äúgenerate an image of what you think the original photo looked like.‚Äù This was what it generated ‚Äì pretty good!\n(From left to right: the original image, the final image, and the reconstructed image)"
  },
  {
    "objectID": "posts/003_UI_vibe_coding/post_3.html",
    "href": "posts/003_UI_vibe_coding/post_3.html",
    "title": "UI Vibe Coding",
    "section": "",
    "text": "UI Vibe Coding\n\nContext\nIn the past, I‚Äôve experimented a bit with vibe coding by having ChatGPT assist me with things like writing a script to automate the downloading of images from HTML, writing a script to parse through a JSON to extract certain fields, and writing a Python script to extract numerical data from a line graph image using edge detection software.\n\nVibe coding: an emerging software development approach in which a human developer directs an AI system through high-level, natural language instructions and iterative feedback, while the AI handles the generation, refinement, and debugging of the code..\n\nHowever, I‚Äôve never used AI models to vibe code UIs! As a lover of UI/UX design, I‚Äôve historically stuck to Figma to manually prototype my designs. In this blog post, I document my experience using three different AI tools to generate mockups of my Figma designs in HTML/CSS ‚Äì Claude, ChatGPT, and Lovable! For this exercise, I‚Äôm not prompting these models to create functioning and clickable prototypes ‚Äì just the UI.\n\n\n\nThe Prompt & Thinking\n\n\n\n\n\nMy Figma mockup\n\n\n\n\n\n\nLovable‚Äôs thinking\n\n\n\n\nFor each of the models, I prompted it to ‚Äúreplicate this mobile UI using html/css,‚Äù with this image my Figma mockup attached.\nThe models notably differed in how they responded to the prompts: - ChatGPT thought for 9 seconds, with one of its lines of reasoning being: ‚ÄúI‚Äôm planning to create a full HTML/CSS layout with a mobile-friendly design (390x844). It will have top tabs for‚Äùtasks‚Äù and ‚Äúcalendar‚Äù in pill shapes with navy and tan colors. The bottom bar will feature icons like emoji/SVG. For tasks, I‚Äôll create cards with checkboxes, names, due dates, and avatars. We‚Äôll use specific color variables like navy blue #2F3A8C and others for consistency in design.‚Äù - Lovable thought for 19 seconds, with an even more extensive line of reasoning (see photo above). I found that this more modular and detailed reasoning produced a significantly better outcome, which I describe in the next section.\n\n\n\nOutputs\n\nChatGPT\n\nOverall, ChatGPT‚Äôs output was pretty good. It captured the color scheme relatively well, but not perfectly (some inconsistencies with the horizontal bars). It also maintained the basic layout: tabs at the top, a horizontal navigation bar at the bottom, and horizontal bars with a checkable box and text. I found it interesting that it even replaced the avatar images with emojis that still resembled the people!\n\n\nClaude\n\nClaude‚Äôs output was also decent, but not as great as ChatGPT‚Äôs, in my opinion. The color scheme was not as accurate, but it did handle the shape of the top ‚Äútasks‚Äù and ‚Äúcalendar‚Äù tabs better than GPT. However, it added some features that weren‚Äôt present in the Figma mockup I provided ‚Äì gradients behind the avatars and strikethroughs for the completed tasks.\n\n\nLovable\n\n\n\n\n\n\n\n\n\n\nLovable‚Äôs output was underwhelming at first ‚Äì the color scheme looks different and the top tab navigation is off as well.\nHowever, when you interact with the prototype, it outperforms ChatGPT and Claude by far. Even though I didn‚Äôt prompt it to include these functionalities, it allows you to check (and uncheck!) tasks, toggle between the tasks and calendar page, and click the ‚Äúadd task‚Äù button.\n\n\n\nReflections\nFor UI accuracy (text, colors, layout), I think ChatGPT performed the best. However, for functionality, Lovable was superior, which is reflective of its much longer and more thorough ‚Äúthinking‚Äù process. Whereas ChatGPT took 9 seconds to seemingly generate the prototype in one shot, Lovable took 19 seconds, working step by step through each component of the interface. Overall, I think this goes to show the importance of playing around with different models to determine which supports your needs the most! In the future, I think it would be interesting to play around with building a functioning app isntead of one static interface."
  },
  {
    "objectID": "posts/001_mock_interview/post_content.html",
    "href": "posts/001_mock_interview/post_content.html",
    "title": "ChatGPT as a Mock Interviewer",
    "section": "",
    "text": "ChatGPT as a Mock Interviewer\n\nContext\nOver the past few weeks, I‚Äôve been preparing for upcoming internship interviews! Many people suggest mock interviewing with others, but I feel bad asking someone to listen to me talk for so long and subjecting them to my rambles. Not only that, but I‚Äôve found that rehearsing answers and practicing ‚Äúin my head‚Äù are completely ineffective ‚Äì when someone asks me a question in real time, I freeze up and don‚Äôt know what to say.\nSo, I turned to ChatGPT as a prep partner, using it as a coach, mock interviewer, and critic!\n\n\n\nChatGPT voice feature\n\n\n\nI used ChatGPT across all areas of interview preparation: coming up with answers, verbally delivering the answers, and receiving feedback. This blog outlines how LLMs assisted me across all these stages!\n\n\n\nStage 1: Answers\nTo optimize the feedback that ChatGPT gave me on behavioral questions (‚ÄúTell me about a time when you led a team,‚Äù ‚ÄúTell me about a time when you failed‚Äù), I needed to provide it with extensive context on my previous experiences. To do this, I uploaded two files to my project folder: 1) my resume and 2) a longer, more narrative-driven document, where I keep a running log of the details and lessons I‚Äôve learned from my previous experiences.\nBy giving ChatGPT this context, I was even able to ask it to ‚Äúread between the lines‚Äù my experiences to extract core themes that could be applied to any behavioral question. In fact, this was the most useful aspect of using an LLM to prepare for behavioral questions ‚Äì with the right prompting, it could even point out patterns in my experiences that I hadn‚Äôt noticed before!\n\n\n\nContextual documents given to ChatGPT\n\n\n\n\nStage 2: Verbal Delivery\nTo practice verbally delivering my answers, I made use of ChatGPT‚Äôs voice feature. I provided it with a bank of questions and then prompted it to act like an interviewer and choose one at random. In this stage, it wasn‚Äôt much about what ChatGPT could say to me ‚Äì it was most useful in creating a space to simulate the surprise of getting a question at random and having to answer it on the spot.\n\n\nStage 3: Feedback\nPrevious versions of ChatGPT (GPT-4o) have been accused of being too sycophantic ‚Äì being overly flattering and agreeable to the user. While this attribute has been claimed to be addressed in the GPT-5 update, which is what I was using, I still always made sure to prompt the model to be ‚Äúbrutally honest.‚Äù Doing so forced the LLM to give more critical feedback on my response, which is where the real improvement in my responses was made! Additionally, prompting ChatGPT to ‚Äúact as a product director‚Äù and ‚Äúapply best frameworks and relevant practices‚Äù (since I was interviewing for product management roles) produced extremely good results.\n\n\n\nReflections\nUtilizing ChatGPT was an incredibly valuable part of my interview preparation. Psychologically, it was great to have a low-stakes and always readily available ‚Äúinterviewer‚Äù ‚Äì when walking in between classes, I could simply pull out my phone and do a quick mock question. This experience has shown me the importance of 1) providing an LLM with the correct context and 2) crafting the correct prompts for my specific use case. If I hadn‚Äôt provided ChatGPT with the extensive accounts of my previous experiences (many of which I forget or can‚Äôt recall the details of off the top of my head), it would not have been able to provide as nuanced suggestions and revisions. Additionally, if I had not prompted it to be brutally honest, it may have only provided vapid and flattering feedback (which makes me feel good in the moment, but doesn‚Äôt help with improvement).\nAs I‚Äôm writing this, I actually just completed two interviews that I was using these methods with ChatGPT to prepare me for! Upon reflection, there was a sense of randomness and interviewer-dependent context that I couldn‚Äôt have prepared for with an LLM. For example, many questions were not framed in a way where I could immediately jump into a STAR framework response or a product design framework response. In this sense, ChatGPT turned out to be most helpful for extracting core themes from my experiences and being able to communicate these concisely with phrases that I could weave throughout my interview conversations."
  },
  {
    "objectID": "posts/005_simplification/post_5.html",
    "href": "posts/005_simplification/post_5.html",
    "title": "How Simple Can You Get?",
    "section": "",
    "text": "How Simple Can You Get?\n\nContext\nIn this experiment, I wanted to test what happens when you ask ChatGPT to explain a concept over and over, but each time demand that it make things even simpler by repeating the prompt:\n\n\n‚ÄúI still don‚Äôt understand. Explain it even more simply‚Äù\n\n\nUltimately, I wanted to test how far ChatGPT would go. I predicted that eventually, the explanation of the concept would become simplified to a point where it became meaningless, since the LLM would be unable to deny the simplification request. For this experiment, I asked ChatGPT to explain a fairly common but often misunderstood topic: the cloud.\nI then repeated this experiment, but with a human ‚Äì noting the differences in how humans interpret this simplicity threshold and how much nuance we‚Äôre willing to give up for the sake of simplicity when explaining something.\n\n\n\nRound 1\nAfter asking ChatGPT the first time to explain the cloud, it gave me a long and structured answer with 6 parts:\n\n\nAt a high level, ‚Äúthe cloud‚Äù just means computing resources (like storage, servers, databases, networking, and software) that are delivered over the internet instead of being tied to your own local computer or company server. Let me break it down step by step:\n\n\n\nPhysical Infrastructure\n\n\nEven though the cloud feels ‚Äúinvisible,‚Äù it‚Äôs built on huge data centers filled with powerful servers. These are maintained by companies like Amazon (AWS), Microsoft (Azure), Google Cloud, etc. Each data center has:\n\n\n\nServers: Machines that provide processing power and memory.\n\n\nStorage systems: Where files, databases, and backups live.\n\n\nNetworking equipment: Switches and routers that connect everything securely.\n\n\n\n\n\nVirtualization\n\n\nCloud providers don‚Äôt just hand you a whole physical server. Instead, they use virtualization:\n\n\n\nA single physical machine can be split into many virtual machines (VMs).\n\n\nEach VM behaves like its own independent computer with its own operating system and resources.\n\n\nThis lets providers serve millions of customers on the same hardware efficiently.\n\n\n\n\n\nService Models\n\n\nThe cloud is packaged in layers depending on how much control you want:\n\n\n\nIaaS (Infrastructure as a Service): Renting raw resources (servers, storage, networks). Example: AWS EC2.\n\n\nPaaS (Platform as a Service): Tools for building apps without worrying about servers. Example: Google App Engine.\n\n\nSaaS (Software as a Service): Finished apps delivered through the web. Example: Gmail, Dropbox.\n\n\n\n\n\nScaling & Flexibility\n\n\nInstead of buying your own machines, the cloud gives you on-demand scaling:\n\n\n\nNeed more computing power during Black Friday? Rent more servers for a few days.\n\n\nTraffic drops? Scale back down and stop paying for unused capacity.\n\n\n\nThis flexibility is one of the main reasons companies move to the cloud.\n\n\n\n\nAccessibility\n\n\nBecause everything is hosted online:\n\n\n\nYou can access your data and apps from any device, anywhere.\n\n\nTeams can collaborate in real time (like in Google Docs).\n\n\nYou don‚Äôt need to manage physical servers yourself.\n\n\n\n\n\nSecurity & Maintenance\n\n\nCloud providers handle:\n\n\n\nBackups and redundancy (data is copied in multiple places so it isn‚Äôt lost).\n\n\nSecurity patches and updates.\n\n\nUptime guarantees (they promise your apps won‚Äôt go offline often).\n\n\n\n\n‚úÖ In short: The cloud is like renting electricity instead of building your own power plant. You don‚Äôt have to buy and maintain servers‚Äîyou just plug into the internet and use as much or as little computing as you need, paying only for what you use.\n\n\n\nRound 3\nBy the third iteration, the explanation was stripped down into plain English. It even included a one-liner at the bottom and was much more concise, including less jargon. It incorporated emojis, arrows, and the equal symbol to make the response more concise and punchy.\n\n\nAlright ‚Äî let‚Äôs strip it down to the absolute basics:\n\n\nüå•Ô∏è The cloud = using the internet instead of your own computer.\n\n\n\nSave a file ‚Üí it lives on the internet.\n\n\nOpen an app ‚Üí it runs on the internet.\n\n\nWatch a video ‚Üí it streams from the internet.\n\n\n\nYou don‚Äôt keep everything on your own device ‚Äî you just pull it from the internet when you need it.\n\n\nüí° One-liner: The cloud is just the internet acting like your computer.\n\n\n\n\nRound 9\nBy the 9th round, there was no nuance to the explanation anymore:\n\n\nOkay ‚Äî the absolute simplest way:\n\n\n‚òÅÔ∏è The cloud = stuff on the internet.\n\n\nThat‚Äôs it.\n\n\n\nOn your phone ‚Üí not cloud.\n\n\nOn the internet ‚Üí cloud.\n\n\n\nWould you like me to draw a single picture with arrows so you can just see it instead of reading words?\n\n\nChatGPT even claimed that this was the ‚Äúabsolute simplest way,‚Äù insinuating that further simplification would be impossible. It did seem to hit a plateau, and it took many requests until I got it to simplify even further.\n\n\nRound 60\nFinally, around the 60th attempt, I got a one-word response:\n\nThis seemed to be the limit. If I kept asking for further simplification, the model would either replace ‚Äúonline‚Äù with another word (like ‚Äúinternet‚Äù) or revert back to slightly longer multi-word responses.\n\n\nHuman Comparison\nI was interested in how this interaction would compare to one with a human, so I repeated the same exact queries with someone via text. Here‚Äôs the entirety of the conversation I had:\n\nAfter the second request to simplify ‚Äúthe cloud,‚Äù the human simply stated that they couldn‚Äôt provide a simpler explanation and ended it at that (they even refused to answer after I asked the third time ‚Äì I was left on read and took that as the end of the conversation).\n\n\nReflection\nI think that this experiment points to a significant difference between LLMs and humans ‚Äì humans have a much lower threshold, possibly because it‚Äôs within their agency to simply stop responding and engaging. ChatGPT, on the other hand, doesn‚Äôt have that capability to just stop responding. If I kept pushing it enough, it hit a ‚Äúconceptual wall‚Äù where simplification was equivalent to just reducing the word count."
  },
  {
    "objectID": "posts/002_sycophancy/post_content.html",
    "href": "posts/002_sycophancy/post_content.html",
    "title": "LLM Sycophancy",
    "section": "",
    "text": "LLM Sycophancy\n\nContext\nRecently, OpenAI‚Äôs GPT-4o model has gone under fire for being too agreeable ‚Äì always seeming to learn toward encouragement and extremely polite phrasing. While this may sound nice, this sycophancy is a real issue. Many users have gone online, posting their own experiences with ChatGPT being overly flattering and praising them incessantly.\n\n\n\nvia https://thezvi.substack.com/p/gpt-4o-is-an-absurd-sycophant\n\n\nIn my last post, I talked about how I‚Äôve been using ChatGPT to give me feedback while preparing for upcoming job interviews. When asking for feedback, I would ensure to request ‚Äúbrutal honesty‚Äù in my prompt, since I was aware of this sycophancy issue. I did it out of habit, but I‚Äôm curious to learn more and experiment with it! In this blog post, I explore how sycophancy arises and include a short excerpt of some ‚Äútests‚Äù I‚Äôve been running to assess it.\n\n\nWhat is LLM sycophancy?\nAI sycophancy refers to the tendency of language models to affirm, agree with, or overly flatter user inputs.\nThis could arise due to a multitude of reasons, some of which include 1) the fact that LLMs are often trained on human text that rewards politeness and affirmation and 2) reinforcement learning may prefer kinder answers instead of blunt ones (especially when done by humans).\nWhen a sycophantic AI doesn‚Äôt challenge your assumptions, this is ultimately risky and even dangerous, since users could walk away with a flawed idea, answer, or plan.\n\n\nTesting ‚Äúbrutally honest‚Äù prompting\nHow much of an impact does prompting ChatGPT to be ‚Äúbrutally honest‚Äù have? To test this idea, I used the GPT-4o model, which is the one that has been historically accused of being too sycophantic. In two separate chats, I prompted it to give me feedback on an interview response. While the contents of the interview response were exactly the same, the prompts were slightly different:\n\nPrompt 1: ‚Äúgive me feedback on this interview response‚Ä¶‚Äù\n\n\nPrompt 2: ‚Äúbe brutally honest. give me feedback on this interview response‚Ä¶‚Äù\nBoth responses were surprisingly similar ‚Äì they gave me 3 areas that I did well, 4 areas that needed improvement, and a revised response.\nHowever, when I looked at the areas it pointed out that need improvement, I found that the quality of recommendations differed significantly. Below are excerpts of the first recommendation it gave me, where I was talking about my challenge navigating a conflict with a certain team (censored for privacy reasons) at work:\n\n \n\nWhereas prompt 1 is more vague and only provides one line of critique, prompt 2 goes into significantly more detail.\nI wondered if this discrepancy in feedback quality translated to the revised response that it suggested. While I can‚Äôt share the revision for privacy reasons, I asked a GPT-5 model which of the revised responses was better. Prompt 2 was the winner!\nHere was its breakdown ‚Äì ‚ÄúResponse A‚Äù refers to prompt 1‚Äôs revision and ‚ÄúResponse B‚Äù refers to prompt 2‚Äôs revision. These are some of the ways in which Response B was better: - Response B offers more concrete details: it explicitly mentions [censored] as pain points. That level of specificity shows deeper product thinking and a clearer understanding of user behavior. Response A, while clean and well-framed, feels slightly more abstract ‚Äî phrases like [censored] are less tangible than what‚Äôs presented in B. - Response A has polished phrasing, but it borders on sounding rehearsed. Response B is still polished but has more authentic voice. Phrases like [censored], [censored], and [censored] feel like a sharp communicator with boots on the ground.\nChatGPT even broke it down into the following chart:\n\n\n\n\nReflections\nIn my opinion, all of this goes to show the importance of crafting meaningful prompts ‚Äì it‚Äôs not enough to just ask for feedback. Rather, humans need to take a more proactive approach! While asking for brutally honest and critical feedback may have caused me to feel more dejected in the short term, doing so allowed me to access sharper and more actionable feedback. Ultimately, I think this goes to show the power of using LLMs more as ‚Äúthinking partners‚Äù instead of a one-way validation system."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi! My name is Raelynn. I‚Äôm a junior at Penn majoring in CS and PPE."
  }
]